# D-Face Hunter ARM64 v1.2.1

**Deterministic Face Hunter** â€“ SystÃ¨me de reconnaissance faciale robuste et transparent, optimisÃ© pour les architectures **ARM64**.  Cette version met Ã  profit l'API **MediaPipeÂ FaceÂ Landmarker** pour extraire 478Â repÃ¨resÂ 3D et introduit une validation sÃ©quentielle multiâ€‘critÃ¨res.

![Python](https://img.shields.io/badge/Python-3.11%20%7C%203.12-blue.svg)
![Platform](https://img.shields.io/badge/Platform-ARM64-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

---

## ğŸ¯ PrÃ©sentation

**Dâ€‘FaceÂ Hunter** est un systÃ¨me dÃ©terministe de reconnaissance faciale conÃ§u pour les appareils ARM64.  Il repose sur lâ€™API **MediaPipeÂ FaceÂ Landmarker** pour extraire **478 repÃ¨res 3D** (468 du maillage facial + 10 dâ€™iris) et calculer la pose (yaw, pitch, roll).  La versionÂ 1.2.1 introduit un mode de **validation sÃ©quentielle multiâ€‘critÃ¨res** pour lâ€™identification en foule (1Â :N) et fournit une documentation complÃ¨te pour comprendre et modifier lâ€™algorithme.

### FonctionnalitÃ©s clÃ©s

* âœ… **478 repÃ¨resÂ 3D**Â : maillage facial complet + iris grÃ¢ce Ã  MediaPipe.
* âœ… **Calcul de pose**Â : extraction dâ€™une matrice 4Ã—4 et conversion en yaw/pitch/roll calibrÃ©s.
* âœ… **Modes de vÃ©rification modulables**Â : *temporal* (DTW), *spatial* (filtrage par pose), *spatiotemporel* (fusion DTW/pose) et *sÃ©quentiel* (multiâ€‘critÃ¨res).
* âœ… **Validation sÃ©quentielle**Â : combinaison de distances normalisÃ©es sur des groupes de repÃ¨res, ratios anthropomÃ©triques, couverture de pose et marge relative pour rÃ©duire les faux positifs en 1Â :N.
* âœ… **EnrÃ´lement en deux phases**Â : capture automatique (frontal/gauche/droite) puis validation manuelle via lâ€™interface interactive.
* âœ… **Scripts interactifs**Â : outils conviviaux pour lâ€™enrÃ´lement et la vÃ©rification.

---

## ğŸ—ï¸ Architecture

Lâ€™architecture complÃ¨te du projet est dÃ©taillÃ©e dans `docs/PIPELINE_OVERVIEW.md`.  En rÃ©sumÃ©, la pipeline comprendÂ :

1. **Capture camÃ©ra** via OpenCV.
2. **DÃ©tection MediaPipe** et extraction de 478 repÃ¨res 3D + pose.
3. **Calibrage et normalisation** des repÃ¨res (PCA, standardisation).
4. **Comparaison** selon quatre modesÂ : temporal, spatial, spatiotemporel ou sÃ©quentiel.
5. **DÃ©cision** basÃ©e sur un seuil et une marge relative (en 1Â :N).

Le document `PIPELINE_OVERVIEW.md` fournit des schÃ©mas et des explications dÃ©taillÃ©es sur chaque Ã©tape.

---

## ğŸ“¦ Installation

### Prerequisites

- **Hardware**: ARM64 device (Raspberry Pi 4/5, Jetson Nano, FuriPhone, etc.)
- **OS**: Linux ARM64 (Debian/Ubuntu based)
- **Python**: **3.12.x OBLIGATOIRE** (3.11 possible mais 3.12 recommandÃ©)
  - âš ï¸ **Python 3.13+ NON COMPATIBLE** avec MediaPipe 0.10.18
  - Installer via pyenv recommandÃ© (voir guide d'installation)
- **Camera**: USB webcam (ex: /dev/video5, /dev/video6) ou CSI camera
- **Display**: Support Qt/XCB pour interface graphique (QT_QPA_PLATFORM=xcb)

### Quick Install

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/D_Face_Hunter_ARM64.git
cd D_Face_Hunter_ARM64_Vers_1_2_final_release

# IMPORTANT: Installer Python 3.12 via pyenv (si pas dÃ©jÃ  installÃ©)
# Voir docs/INSTALLATION.md pour installation complÃ¨te de pyenv

# CrÃ©er environnement virtuel avec Python 3.12
~/.pyenv/versions/3.12.12/bin/python -m venv mp_env
source mp_env/bin/activate

# Installer les dÃ©pendances (avec contraintes de version strictes)
pip install --upgrade pip
pip install opencv_whl_4_12/opencv_contrib_python-4.12.0-py3-none-linux_aarch64.whl
pip install "numpy<2.0" mediapipe==0.10.18 scipy scikit-learn dtaidistance

# TÃ©lÃ©charger le modÃ¨le MediaPipe (si non inclus)
mkdir -p models/mediapipe
wget -O models/mediapipe/face_landmarker.task \
  https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task
```

**âš ï¸ ATTENTION**: Ne pas utiliser Python 3.13, MediaPipe n'est pas compatible !

Pour une installation guidÃ©e complÃ¨te : **voir `docs/INSTALLATION.md`**

### Verify Installation

```bash
source mp_env/bin/activate
python -c "import mediapipe; print('MediaPipe:', mediapipe.__version__)"
# Expected: MediaPipe: 0.10.18

python -c "import numpy; print('NumPy:', numpy.__version__)"
# Expected: NumPy: 1.26.4 (DOIT Ãªtre < 2.0)

python -c "import cv2; print('OpenCV:', cv2.__version__)"
# Expected: OpenCV: 4.12.0

python -c "from src.fr_core import VerificationDTW; print('âœ… D-Face Hunter ready')"
# Expected: âœ… D-Face Hunter ready
```

---

## ğŸš€ Quick Start

### 1. Enroll a User

```bash
# Activer l'environnement
source mp_env/bin/activate
export QT_QPA_PLATFORM=xcb

# Interface tactile (recommandÃ© pour smartphone/tablette)
python enroll_touchscreen.py
# OU lancer avec : ./launch_touchscreen.sh

# Interface clavier (pour PC/laptop)
python enroll_interactive.py

# Enrollment direct (ligne de commande)
python scripts/enroll_landmarks.py <username> --camera 5
```

**Enrollment Process:**
1. **Phase 1** (Automatic - 45 frames):
   - Look straight at camera (frontal: 15 frames)
   - Turn head left (left: 15 frames)
   - Turn head right (right: 15 frames)
   - System auto-captures frames when pose changes

2. **Phase 2** (Manual - 5+ frames):
   - Press **SPACE** to capture each frame
   - Vary your pose for robustness
   - Press **'q'** when done (minimum 5 frames)

3. **Validation** (Immediate test):
   - Stay in front of camera
   - System verifies enrollment works
   - Shows distance and coverage

**Output**: `models/users/<username>.npz` (landmarks + poses)

### 2. Verify Identity

```bash
# Activer l'environnement
source mp_env/bin/activate
export QT_QPA_PLATFORM=xcb

# Interactive verification (recommended)
python verify_interactive.py

# Or direct verification
python scripts/verify_mediapipe.py models/users/<username>.npz --camera 5 --seconds 5
```

**Verification Process:**
- Captures 5 seconds of video (~30-45 frames)
- Compares with enrolled user using spatial mode
- Returns: Match (YES/NO), Distance, Coverage

**Expected Output:**
```
âœ… User: john_doe
âœ… Verified: YES
âœ… Distance: 1.234567 (< 3.0 threshold)
âœ… Coverage: 45.2%
```

---

## ğŸ“ Project Structure

```
D_Face_Hunter_ARM64_Vers_1_2_sameperson/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ setup_env.sh                 # Environment setup script
â”œâ”€â”€ enroll_interactive.py        # Interactive enrollment
â”œâ”€â”€ verify_interactive.py        # Interactive verification
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ fr_core/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py            # Configuration
â”‚       â”œâ”€â”€ verification_dtw.py  # Spatial matching engine
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ enroll_landmarks.py      # Enrollment script (MediaPipe)
â”‚   â”œâ”€â”€ verify_mediapipe.py      # Verification script (spatial mode)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ camera_calibration.json  # Offsets de pose (optionnel)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mediapipe/
â”‚   â”‚   â””â”€â”€ face_landmarker.task
â”‚   â””â”€â”€ users/                   # Enrolled users (.npz files)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PIPELINE_OVERVIEW.md     # Description du pipeline
â”‚   â”œâ”€â”€ VALIDATION_CRITERIA.md   # CritÃ¨res de validation et seuils
â”‚   â”œâ”€â”€ MODES.md                 # Description des modes de comparaison
â”‚   â”œâ”€â”€ INSTALLATION.md          # Guide dâ€™installation dÃ©taillÃ©
â”‚   â””â”€â”€ TESTS.md                 # Description des tests
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_imports.py          # VÃ©rification des imports
    â”œâ”€â”€ test_enrollment_and_verification.py  # Tests synthÃ©tiques 1:1 et 1:N
    â””â”€â”€ data/                    # DonnÃ©es de test (optionnel)
```

---

## âš™ï¸ Configuration

Le systÃ¨me nâ€™utilise pas de fichier YAML de configurationÂ : toutes les options sont dÃ©finies dans les classes Python `Config` et `ConfigSequential` situÃ©es dans le module `src/fr_core`. Ces classes sont dÃ©clarÃ©es sous forme de dataclasses et chargent des paramÃ¨tres par dÃ©faut.

Vous pouvez modifier ces paramÃ¨tres de deux faÃ§onsÂ :

1. **En modifiant les attributs du dataclass avant dâ€™instancier le vÃ©rificateur**. Par exempleÂ :

```python
from src.fr_core.config import Config

config = Config()
config.matching_mode = "spatial"           # ou "temporal", "spatiotemporal", "sequential"
config.pose_epsilon_yaw = 10.0             # tolÃ©rance sur lâ€™angle de lacet (en degrÃ©s)
config.pose_epsilon_pitch = 10.0           # tolÃ©rance sur lâ€™angle de tangage
config.pose_epsilon_roll = 10.0            # tolÃ©rance sur lâ€™angle de roulis
config.pose_threshold = 3.0                # seuil de distance pour accepter un match en mode spatial

# paramÃ¨tres pour lâ€™algorithme sÃ©quentiel
config.weight_invariant = 0.4
config.weight_stable    = 0.3
config.weight_pose      = 0.2
config.weight_ratio     = 0.1
config.composite_threshold = 0.8
config.composite_margin    = 0.2
config.coverage_threshold  = 0.3
config.coverage_margin     = 0.2
```

2. **En passant des arguments au niveau des scripts**. Les scripts `enroll_interactive.py` et `verify_interactive.py` acceptent des options en ligne de commande (par exemple `--matching-mode`, `--pose-epsilon-yaw`, etc.) qui Ã©crasent les valeurs par dÃ©faut du dataclass.

3. **Via lâ€™interface tactile (`launch_touchscreen.py`)**Â : lorsque vous exÃ©cutez le script `launch_touchscreen.py`, un bouton **PARAMETRES** sâ€™affiche dans le menu principal.  Il ouvre un Ã©cran de rÃ©glage qui permet dâ€™ajuster les principaux seuils (DTW, pose, spatiotemporel, composite) ainsi que les marges et la couverture au moyen de boutons `+` et `â€“`.  Les valeurs sÃ©lectionnÃ©es sont enregistrÃ©es dans le fichier `config/user_config.json` via `save_user_config()` et sont automatiquement rÃ©appliquÃ©es Ã  chaque lancement.

4. **Via le script en ligne de commande `scripts/settings_cli.py`**Â : ce script permet de modifier les paramÃ¨tres depuis le terminal sans passer par lâ€™interface graphique.  Par exempleÂ :

```bash
python scripts/settings_cli.py \
    --composite_threshold 0.8 \
    --composite_margin 0.2 \
    --coverage_threshold 0.3 \
    --coverage_margin 0.2
```

Le script prend en charge plusieurs arguments correspondant aux attributs de la classe `Config` (voir `scripts/settings_cli.py --help` pour la liste complÃ¨te).  Les modifications sont enregistrÃ©es dans `config/user_config.json`.  Utilisez `--reset` pour supprimer ce fichier et revenir aux valeurs par dÃ©faut.

Les paramÃ¨tres disponibles sont dÃ©crits en dÃ©tail dans les fichiers `docs/VALIDATION_CRITERIA.md` et `docs/MODES.md`.

---

## ğŸ§ª Tests

Le dossier `tests/` contient des tests unitaires et fonctionnels basÃ©s sur `pytest`. Pour lancer tous les testsÂ :

```bash
cd D_Face_Hunter_ARM64_Vers_1_2_sameperson
pytest -q
```

Les scripts de tests principaux sontÂ :

- **`test_imports.py`**Â : vÃ©rifie la prÃ©sence des dÃ©pendances (MediaPipe, numpy, etc.).
- **`test_enrollment_and_verification.py`**Â : effectue des tests 1:1 (mÃªme personne) et 1:N (galerie) sur des donnÃ©es synthÃ©tiques pour valider les diffÃ©rentes mÃ©thodes de comparaison.
- **Autres tests**Â : la plupart des fichiers de test originaux sont conservÃ©s pour valider le fonctionnement de MediaPipe, lâ€™alignement des repÃ¨res et la cohÃ©rence de la pose.

Les instructions dÃ©taillÃ©es pour reproduire les scÃ©narios de test (y compris les cas imposteur) sont dÃ©crites dans `docs/TESTS.md`.

---

## ğŸ“Š Performances et prÃ©cision

Les temps de traitement varient selon la plateforme. Ã€ titre indicatif, sur une RaspberryÂ PiÂ 5 (ARM64)Â :

| OpÃ©ration                 | Temps approximatif | Notes                            |
|---------------------------|--------------------|----------------------------------|
| EnrÃ´lement (phaseÂ 1+2)    | 15â€“20Â s            | 90Â images capturÃ©es              |
| VÃ©rification              | 0,2â€“0,5Â s          | SÃ©quence probe de 30Â frames      |
| Appel du vÃ©rificateur     | 0,05Â s             | Par comparaison 1:1 ou 1:N       |

Les valeurs de distance et de couverture dÃ©pendent de lâ€™utilisateur et du modeÂ :
- **AutovÃ©rification**Â : distance proche de 0â€“2 (correspondance parfaite).  
- **MÃªme personne, autre session**Â : distance typiquement entre 1 et 3.  
- **Personnes diffÃ©rentes**Â : distance supÃ©rieure Ã  3 (rejeter).

Dans le mode sÃ©quentiel, on calcule un **score composite** normalisÃ©. Ce score doit Ãªtre infÃ©rieur au seuil (`composite_threshold`) et la diffÃ©rence relative entre le meilleur et le second score doit dÃ©passer `composite_margin` pour valider une identitÃ©. La **couverture** (proportion de frames comparables) doit Ã©galement Ãªtre supÃ©rieure Ã  `coverage_threshold`.

---

## ğŸ”¬ DÃ©tails techniques

### MediaPipe Integration

```python
# Face detection + 468 landmarks + pose
import mediapipe as mp
from mediapipe.tasks.python import vision

detector = vision.FaceLandmarker.create_from_options(options)
result = detector.detect(image)

# Extract landmarks (468 points)
landmarks = result.face_landmarks[0][:468]  # (x, y, z)

# Extract pose from transformation matrix
pose_matrix = result.facial_transformation_matrixes[0]  # 4Ã—4 matrix
rotation = Rotation.from_matrix(pose_matrix[:3, :3])
yaw, pitch, roll = rotation.as_euler('XZY', degrees=True)  # Euler XZY convention
```

### Spatial Matching Algorithm

```python
def verify_pose_based(probe_landmarks, probe_poses, 
                     gallery_landmarks, gallery_poses):
    """
    Spatial pose-aware matching.
    
    For each probe frame:
      1. Find gallery frames with similar pose (epsilon filtering)
      2. Compute Euclidean distance to each similar frame
      3. Keep minimum distance
    
    Average all per-frame minimum distances â†’ Final score
    """
    distances = []
    for i, probe_frame in enumerate(probe_landmarks):
        # Filter gallery by pose similarity
        similar_frames = find_similar_poses(
            probe_poses[i], gallery_poses,
            epsilon_yaw=10.0, epsilon_pitch=10.0, epsilon_roll=10.0
        )
        
        if len(similar_frames) > 0:
            # Compute distances to similar frames
            dists = [euclidean_distance(probe_frame, gallery_landmarks[j]) 
                     for j in similar_frames]
            distances.append(min(dists))
    
    return np.mean(distances)
```

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

**1. MediaPipe not found**
```bash
pip3 install mediapipe==0.10.18
```

**2. Camera not opening**
```bash
# Check camera device
ls -l /dev/video*

# Test with OpenCV
python3 -c "import cv2; cap = cv2.VideoCapture(0); print('Camera:', cap.isOpened())"
```

**3. Validation returns `distance: inf`**
- Ensure you're using the latest version with spatial mode
- Check that `config.matching_mode = "spatial"`
- Verify enrollment saved poses: `python3 -c "import numpy as np; d = np.load('models/users/<user>.npz'); print('Poses:', 'poses' in d)"`

**4. Low coverage (<10%)**
- Move your head more during verification
- Ensure good lighting
- Check pose ranges match enrollment

---

## ğŸ“ License

MIT License - See LICENSE file for details

---

## ğŸ‘¤ Author

**Jean-Philippe (j-phi)**
- GitHub: [@YOUR_GITHUB_USERNAME]
- Project: D-Face Hunter ARM64

---

## ğŸ™ Acknowledgments

- **MediaPipe** (Google) - Face mesh and pose estimation
- **scikit-learn** - PCA and machine learning tools
- **OpenCV** - Computer vision library
- **DTAIDistance** - Fast DTW implementation (optional)

---

## ğŸ“š Citation

If you use D-Face Hunter in your research, please cite:

```bibtex
@software{dface_hunter_arm64,
  title={D-Face Hunter ARM64: Deterministic Face Recognition for ARM64 Devices},
  author={Jean-Philippe},
  year={2025},
  version={1.2.1},
  url={https://github.com/YOUR_USERNAME/D_Face_Hunter_ARM64}
}
```

---

## ğŸ”® Roadmap

- [x] Multiâ€‘user verification (1:N matching) â€“ implÃ©mentÃ© dans la v1.2.1 via le mode sÃ©quentiel multiâ€‘critÃ¨res.
- [ ] Antiâ€‘spoofing (liveness detection)
- [ ] GPU acceleration (OpenCL)
- [ ] Realâ€‘time continuous monitoring
- [ ] Web interface
- [ ] Mobile app (Android/iOS)

---

**Version:** 1.2.1  
**Last Updated:** JanuaryÂ 01,Â 2026  
**Status:** Release Candidate âœ…
